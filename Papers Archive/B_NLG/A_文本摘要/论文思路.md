

# 论文思路

所读论文：

> [^1]:  2017 **Get To The Point: Summarization with Pointer-Generator Networks** [pdf](./Get To The Point_Summarization with Pointer-Generator Networks.pdf)  [md](./Get To The Point_Summarization with Pointer-Generator Networks.md)
>
> > www.github.com/abisee/pointer-generator
>
>
> [^2]: 2022 **BRIO: Bringing Order to Abstractive Summarization** [pdf](./BRIO_Bringing Order to Abstractive Summarization.pdf)  [md](./BRIO_Bringing Order to Abstractive Summarization.md) 
>
> > https://github.com/yixinL7/BRIO
>
> [^3]: 2021 **SimCLS: A Simple Framework for Contrastive Learning of Abstractive Summarization** [pdf](./SimCLS_A Simple Framework for Contrastive Learning of Abstractive Summarization.pdf) [md](./SimCLS_A Simple Framework for Contrastive Learning of Abstractive Summarization.md)
>
> > https://github.com/yixinL7/SimCLS 	
>
> [^4]: 2020 **PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization**
>
> [^5]: 2019 **Bridging the Gap between Training and Inference for Neural Machine Translation**
>
> 



PEGASUS[^4]

性能上的大幅跳跃表明，小的文本摘要数据集从预训练中获益最大。我们在章节中进一步研究了低资源的总结



之前的一些研究表明，最大似然训练会导致模型输出中的重复文本（wellelketal.，2019）



choosing perplexity-optimized models using aggregated ROUGE：我们发现，即使是低胭脂度模型的总结也往往是高质量的，





尝试的另一个方向是句子级训练，认为句子级度量，例如BLEU，为生成带来了一定程度的灵活性，因此对于减轻暴露偏差问题更鲁棒。[^5]





