# 1. Summarization

所读论文：

> [^1]:  2017 **Get To The Point: Summarization with Pointer-Generator Networks** [pdf](./Get To The Point_Summarization with Pointer-Generator Networks.pdf)  [md](./Get To The Point_Summarization with Pointer-Generator Networks.md)
>
> > www.github.com/abisee/pointer-generator
>
>
> [^2]: 2022 **BRIO: Bringing Order to Abstractive Summarization** [pdf](./BRIO_Bringing Order to Abstractive Summarization.pdf)  [md](./BRIO_Bringing Order to Abstractive Summarization.md) 
>
> > https://github.com/yixinL7/BRIO
>
> [^3]: 2021 **SimCLS: A Simple Framework for Contrastive Learning of Abstractive Summarization** [pdf](./SimCLS_A Simple Framework for Contrastive Learning of Abstractive Summarization.pdf) [md](./SimCLS_A Simple Framework for Contrastive Learning of Abstractive Summarization.md)
>
> > https://github.com/yixinL7/SimCLS 	
>
> [^4]: 2020 **PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization**
>
> 
>
> 

## 1.1 Get To The Point[^1]

* **Get To The Point: Summarization with Pointer-Generator Networks**[^1]

  Abigail See, Peter J. Liu; ACL 2017 ([PDF](.\A_文本摘要\Get To The Point_Summarization with Pointer-Generator Networks.pdf)) ([Markdown](.\A_文本摘要\Get To The Point Summarization with Pointer-Generator Networks.md))  (Citations:2744)

  > 1. 摘要有时不准确地再现事实细节
  > 2. 摘要有时会重复自己

  > **pointer-generator**：copy words from the source text via pointing；copy out-of-vocabulary words from the source text
  > **coverage**：keep track of what has been summarized,which discourages repetition

  <img src="https://cdn.jsdelivr.net/gh/xin-fight/note_image@main/img/image-20230424193209520.png" alt="image-20230424193209520" style="zoom: 50%;" />

## 2.2 BRIO & SimCLS[^2][^3]

* **SimCLS: A simple framework for contrastive learning of abstractive summarization**[^3]

  Yixin Liu, Pengfei Liu; ACL 2021  ([PDF](.\A_文本摘要\SimCLS_A Simple Framework for Contrastive Learning of Abstractive Summarization.pdf)) ([Markdown](.\A_文本摘要\SimCLS_A Simple Framework forContrastive Learning of Abstractive Summarization.md)) (Citations:96)

  > 1. 目标函数和评估度量之间差距
  >    因为目标函数是基于本地的、令牌级别的预测
  >    而评估度量（例如ROUGE）将比较黄金参考和系统输出之间的整体相似性
  >
  > 2. 暴露偏差exposure bias

  > 通过将文本生成表述为一个无参考摘要的评价问题（即质量估计），在对比学习的辅助下，弥合学习目标和目前占主导地位的序列到序列学习框架所产生的评价指标之间的差距。
  > **Candidate Generation**：生成多个候选摘要
  > **Reference-free Evaluation**：更好的候选摘要 应该获得更高的质量分数

  <img src="https://cdn.jsdelivr.net/gh/xin-fight/note_image@main/img/image-20230424194008814.png" alt="image-20230424194008814" style="zoom: 50%;" />

* **BRIO: Bringing order to abstractive summarization**[^2]

  Yixin Liu,Pengfei Liu; ACL 2022 ([PDF](.\A_文本摘要\BRIO_Bringing Order to Abstractive Summarization.pdf)) ([Markdown](.\A_文本摘要\BRIO Bringing Order to Abstractive Summarization.md)) (Citations:41)
  
  > 1. 模型分数的顺序与评估摘要的实际质量指标应该相协调(coordinated ):
  >     更好候选摘要分配更高估计概率
  > 2. exposure bias
  
  > **exposure bias**：为了在模型生成有错误的子序列中保持合理的性能，我们认为该模型必须准确地估计不同生成输出的相对质量，因为有效的推断需要在这些候选序列之间进行比较
  > **Coordinating Abstractive Models**：一个候选摘要的概率应该 与 自动度量M评估的质量密切相关。
  
  <img src="https://cdn.jsdelivr.net/gh/xin-fight/note_image@main/img/image-20230424194228742.png" alt="image-20230424194228742" style="zoom:50%;" />

<hr>

 <style type="text/css">
.tg  {border-collapse:collapse;border-spacing:0;}
.tg td{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
  overflow:hidden;padding:10px 5px;word-break:normal;}
.tg th{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
  font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}
.tg .tg-c3ow{border-color:inherit;text-align:center;vertical-align:top}
</style>
<table>
<thead>
  <tr>
    <th>模型</th>
    <th>SimCLS</th>
    <th>BRIO</th>
  </tr>
</thead>
<tbody>
  <tr>
    <td>相同</td>
    <td colspan="2">产生多个候选摘要，使用对比学习对这些候选排序，使用了两阶段模型：生成和评估<br>解决exposure bias问题</td>
  </tr>
  <tr>
    <td>不同</td>
    <td colspan="2">损失函数定义不同<br>在候选生成和评分方面使用了相同的模型体系结构（BART）</td>
  </tr>
</tbody>
</table>


 # 2. Datasets

> CNN/DailyMail, Multi-News, arXiv, PubMed, BIGPATENT datasets contain input documents longer than the maximum input length (*L**input* = 512 tokens in pretraining.
>
> BIGPATENT, arXiv, PubMed and Multi-News datasets contain very long summaries

## 2.1 CNN/Daily Mail dataset(**CNNDM**)[^1][^2][^3][^4]

 **large-scale datasets**

> 提出论文和网址：
>
> **Teaching machines to readand comprehend** Hermann 2015： https://cs.nyu.edu/~kcho/DMQA/

（Hermann et al.，2015）数据集包含来自CNN的93k篇文章和《每日邮报》的22万篇文章。这两家出版商都用要点摘要来补充他们的文章。我们使用了See等人（2017）中使用的非匿名变体。

which contains **online news articles** (781 tokens on average) paired with **multi-sentence summaries** (3.75 sentences or 56 tokens on average)

> Teaching machines to read and comprehend
>
> Sentence reduction for automatic text summarization



<hr>


<img src="https://cdn.jsdelivr.net/gh/xin-fight/note_image@main/img/image-20230418165600270-1682335782466-1.png" alt="image-20230629030951537" style="zoom:80%;" />

## 2.2 XSum[^2][^3]

XSum is a **highly abstractive dataset of article**s from the British Broadcasting Corporation **(BBC)**.

(Narayan et al., 2018) 内容包括从2010年至2017年的22.7k篇BBC文章，涵盖了广泛的主题，以及专业撰写的单句摘要。

> 提出论文和网址：
>
> **Don’t give me the details, just the summary! topic-aware convolutional neural networks for extreme summarization**. 2018：https://github.com/EdinburghNLP/XSum



## 2.3 NYT[^2]

NYT contains articles from the **New York Times and the associated summaries**.

> 提出论文和网址：
>
> **The New York Times Annotated Corpus** 2008：https://catalog.ldc.upenn.edu/LDC2008T19



## 2.4 Reddit TIFU[^4]

(Kim et al., 2019) 包含了在线论坛Reddit的12万篇非正式文章，更具体地说，是2013年1月至1月至2018年3月的TIFU子论坛。**sub-reddit的帖子严格遵循写描述性的“TL；DR”总结的规则**，质量高于我们的人工检查（V¨olskeetal.，2017）（其使用更多的subreddits）。在工作中使用了TIFU-long的子集（使用TLDR作为摘要）。

its signifificant difference in style：该数据集写作分格多样

## 2.5 NEWSROOM[^4]

(Grusky et al., 2018) is a large dataset containing 1.3M article-summary pairs written by authors and editors in the newsrooms of 38 major publications between 1998 and 2017.

## 2.6 Multi-News[^4]

 （Fabbri et al.，2019）是一个多文档摘要数据集，由56k对新闻文章及其来自newser.com网站的人工编写的摘要组成。

## 2.7 Gigaword[^4]

Gigaword（Rush等人，2015年）包含了从Gigaword语料库中的新闻文章（7家出版商）中提取的400万个例子（Graff等人，2003年）。其任务是从第一句话中生成标题。

## 2.8 arXiv, PubMed[^4]

（Cohan et al.，2018）是来自arXiv.org（113k）和PubMed（215k）的科学出版物的两个长文档数据集。其任务是从论文主体中生成摘要

## 2.9 BIGPATENT[^4]

（Sharma等人，2019年）包括130万项美国专利以及9个专利分类类别下的人类摘要。

## 2.10 WikiHow[^4]

（Koupaee & Wang，2018）是一个来自在线WikiHow.com网站的大规模指令数据集。每一个200k个示例都由多个指令步骤段落和一个总结的句子组成。任务是从段落中生成连接的总结句。

## 2.11 AESLC[^4]

（Zhang&Tetreault，2019）包括180个电子邮件主体及其主题，来自 Enron corpus（Klimt & Yang，2004），这是Enron Corporation员工的电子邮件信息集合。

## 2.12 BillSum[^4]	

(Kornilova & Eidelman, 2019) 包含2.13万份美国国会法案和第103-115届（1993-2018年）国会会议的人工书面参考摘要。

# 3. Evaluation Metrics

## 3.1 **ROUGE**[^1][^2][^3][^4] (Lin 2004), 

> 参考网站：NLP评估指标之ROUGE https://zhuanlan.zhihu.com/p/504279252

> 由于任务的主观性和有效总结的多样性，ROUGE似乎奖励了安全的策略，如选择第一次出现的内容，或保留原始的措辞
>
> 
>
> reporting the F1 scores for **ROUGE-1, ROUGE-2 and ROUGE-L** (which respectively measure the **word-overlap**, **bigram-overlap**, and **longest common sequence** between the reference summary and the summary to be evaluated)
>
> python包：ROUGE scores using the pyrouge package



> ROUGE-1 F1是自然语言处理中用于评估文本摘要质量的一种指标。ROUGE代表"Recall-Oriented Understudy for Gisting Evaluation"，它主要衡量自动摘要生成系统生成的摘要与参考摘要之间的相似度。
>
> ROUGE-1 F1**针对单个词的重叠进行评估，即对于生成的摘要中的每个词，它是否在参考摘要中出现**。ROUGE-1 F1计算两个重要的指标：召回率（Recall）和精确度（Precision）。
>
> 召回率衡量生成的摘要中有多少词出现在参考摘要中，而精确度衡量参考摘要中有多少词出现在生成的摘要中。F1值是召回率和精确度的加权平均值，它综合考虑了两者之间的平衡。
>
> 具体计算ROUGE-1 F1的过程是先计算生成的摘要和参考摘要之间的召回率和精确度，然后使用下面的公式计算F1值：
>
> F1 = (2 * 召回率 * 精确度) / (召回率 + 精确度)
>
> ROUGE-1 F1值越高，表示生成摘要与参考摘要之间的相似度越高，表明摘要生成系统的性能越好。这种评估指标常用于自动摘要任务的评估和比较不同模型的性能。



### 3.1.1 Ind/Seq and Uniq/Orig

> 论文[^4]中使用：
>
> > ==scored independently (**Ind**) / sequentially (**Seq**)==
> >
> > **在独立评分（Ind）的方法中，每个句子都被单独评分，而不考虑其他句子的信息**。然后，根据得分选择排名靠前的m个句子作为摘要的一部分。这种方法忽略了句子之间的相互关系，仅基于单独的得分来选择句子。
> >
> > 与之相对应的是按顺序逐个选择句子的方法（Seq）。在这种方法中，**句子的选择是顺序进行的，每次选择一个句子后，会考虑前面已选择的句子对后续句子的影响。根据句子之间的相互关系和整体的一致性，逐步选择句子来构建摘要**。
> >
> > 这两种方法各有优劣。独立评分（Ind）简单快速，但可能无法捕捉到句子之间的关联信息。按顺序选择（Seq）可以更好地考虑句子之间的连贯性和一致性，但可能会增加计算复杂性和时间开销。
> >
> > 在具体应用中，根据任务的需求和数据的特点，选择适合的方法可以获得更好的摘要结果。有时可以结合两种方法，利用它们的优势来提升摘要的质量和效果。
>
> > ==**Uniq** / **Orig**==
> >
> > 在计算ROUGE-1 F1分数时，我们将n-gram视为一个集合（Uniq），而不像原始实现（Orig）中那样重复计数相同的n-gram。
> >
> > ROUGE-1 F1分数是ROUGE评估指标中的一种，用于衡量生成的摘要与参考摘要之间的一元组（单个单词）重叠情况。在传统的ROUGE-1实现中（Orig），相同的n-gram会被重复计数，可能导致重复的单词在分数计算中占据较大比例。
> >
> > 为了避免这种重复计数的情况，一些改进的方法**（如Uniq）将n-gram视为一个集合，即每个n-gram只计数一次。这意味着无论一个n-gram在生成摘要和参考摘要中出现多少次，它只会被计数一次。**
> >
> > 这种做法旨在更准确地衡量摘要的质量，尤其是当生成的摘要和参考摘要中存在大量重复的单词时。通过将n-gram视为集合，我们避免了重复计数的情况，从而更准确地计算了重叠的n-gram数量，进而计算出ROUGE-1 F1分数。
> >
> > 需要注意的是，不同的ROUGE实现可能采用不同的计数方式（如Orig或Uniq），具体取决于评估的要求和实际应用场景。选择适合您需求的计数方式以确保结果的准确性和一致性。



### 3.1.2 perplexity-optimized models using aggregated ROUGE

>
> 使用聚合ROUGE指标优化的困惑度模型指的是基于困惑度得分进行优化的语言模型，并使用聚合ROUGE指标进行评估。
>
> 困惑度是一种常用的衡量语言模型质量的指标，它量化了语言模型对给定单词序列的预测能力。较低的困惑度值表示更好的性能，因为模型能够将更高的概率赋予实际单词序列。
>
> 另一方面，ROUGE（Recall-Oriented Understudy for Gisting Evaluation）是一组常用于文本摘要任务的评估指标。ROUGE根据各种准则（如n-gram匹配和序列相似性）衡量生成摘要与参考摘要之间的重叠度。
>
> 在“使用聚合ROUGE指标优化的困惑度模型”背景下，模型在语言建模任务中被训练以优化困惑度得分。训练后，使用聚合ROUGE指标对模型进行评估，以评估其在文本摘要或相关任务中的性能。
>
> 聚合ROUGE指标是多个ROUGE指标（如ROUGE-1、ROUGE-2和ROUGE-L）的组合。通过聚合这些指标，可以更全面地评估模型的摘要质量，考虑到单词和句子匹配以及摘要的整体连贯性。
>
> 在优化困惑度的同时，使用聚合ROUGE指标评估模型可以实现平衡的方法，即通过困惑度优化改进模型的语言建模能力，同时使用ROUGE指标评估其摘要性能。
>
> 总的来说，使用聚合ROUGE指标优化的困惑度模型旨在在语言建模质量和文本摘要性能之间取得平衡，提供能够生成连贯摘要并保持良好语言建模能力的模型。



## 3.2 **METEOR**[^1] (Denkowski and Lavie, 2014)

> 它不仅奖励精确的**单词匹配**，而且还奖励匹配的**词干、同义词和释义**（来自一个预定义的列表）
>
> 
>
> both in exact **match mode** (rewarding only exact matches between words只奖励单词之间的精确匹配) and full mode (which additionally rewards matching stems, synonyms and paraphrases 还奖励匹配的词干，同义词和释义)：www.cs.cmu.edu/~alavie/METEOR



## 3.3 **BERTScore**[^2][^3] (Zhang et al., 2020) 

https://github.com/Tiiiger/bert_score

> a model-based semantic similarity metric 基于模型的语义相似度度量



## 3.4 MoverScore[^3]



# 3* 其他领域指标 

## 机器翻译指标之BLEU

参考资料：

> Metric评价指标-机器翻译指标之BLEU:https://zhuanlan.zhihu.com/p/350596071











# 4. Fine-grained Analysis

与基线模型相比，新提出的模型有哪些优势

##  4.1 Entity-level[^3]

受Gekhman等人（KoBE: Knowledge-based machine translation eval uation）（2020）和Jain等人（SciREX: A challenge dataset for document-level information extraction）（2020）工作的启发，我们比较了模型性能***salient entities*** — 这些实体是**出现在参考摘要中 的那些原文档中的实体**。

> (1)我们从源文档中提取实体（use a pre-trained NER model provided by spaCy to extract the entities）
>
> (2)根据参考摘要中的实体去选择 *salient entities*，
>
> (3)将*salient entities* 与 候选摘要中的实体进行比较。



## 4.2 Sentence-level[^3]

**Sentence Alignments**

> (1)我们根据**相似度**（用 ROUGE 分数表示）将摘要中的每个句子与源文档中的句子进行匹配
>
> (2)根据**源文档中 匹配句子 的重叠部分**，计算**参考摘要 和 系统生成的摘要**之间的句子级相似性。



**Positional Bias** - 关于句子对齐(sentence alignment)的研究

> 我们使用相同的匹配方法（用 ROUGE 分数表示）将摘要句子映射到源文章中的句子。
>
> 在这个例子中，我们的方法的输出集中在与参考摘要相同的句子上，而基线摘要集中在一些不同的句子上。



## 4.3 EFFECT OF VOCABULARY[^4]

We compared **two tokenization methods**（ https://github.com/google/sentencepiece） : ==Byte-pair encoding algorithm (**BPE**)== (Wu et al., 2016; Sennrich et al., 2016), and ==SentencePiece Unigram algorithm (**Unigram**)== proposed in Kudo (2018).
