# Can Unconditional Language Models Recover Arbitrary Sentences?

**现在有通用的句子编码器，是否有可能以类似的方式调整它们，==以用作 通用解码器==？**

要使这一点成为可能，对于任何感兴趣的目标句子，都需要有一些**连续的表示**可以传递给语言模型，从而使它重现该句子。

> 忽略如何产生这种表示的编码器；**而是直接询问 <font color='red'>这种传给模型生成句子的表示是否存在</font>**

为此研究这个问题，我们引入了一对有效的、互补的方法来**将  表示 输入到 预先训练过的无条件语言模型中**，以及一组相应的方法来**将句子 映射到 这个表示空间**，即 <font color='red'>重新参数化的句子空间==（the *reparametrized* *sentence space*.）==</font> 

>作者研究了在何种条件下，语言模型可以通过 **识别 该空间（重新参数化的句子空间)）中的一个点 来生成一个句子**。
>
>作者发现，只要使用**适当大小的语言模型和表示**，就几乎可以完美地恢复任意的句子。



***general-purpose encoders*:**

>我们最近看到，在使用预训练好的语言模型 作为一系列困难的自然语言处理任务的 编码器方面取得了巨大的成功
>
>使用 预训练过的语言模型作为一系列困难的自然语言处理任务的编码器**通常很少或没有微调**：==语言模型学习有用的表示，使它们能够作为通用编码器(general-purpose encoders)。== 

***A hypothetical general-purpose decoder* :**

> 可以 为使用很少注释数据的 文本生成任务训练模型，并在内存有限的环境中在各个应用程序中 广泛地共享参数。
>
> 为了使其成为可能：
>
> >  <font color='red'>continuous sentence representation </font> - 将某种形式的 **连续句子表示** 输入到训练过的语言模型中
> >
> >   <font color='red'>a task-specifific encoder </font> - 将一些**任务输入 转换为 一个句子表示，从而使语言模型产生所需的句子**

我们不知道有任何工作已经成功地产生了可以以这种方式与预训练语言模型互操作的编码器；在本文中，我们询问是否有可能：典型的、训练有素的**神经网络语言模型能够 通过这种调节 恢复任意句子**



***reparametrized sentence space*  - <font color='red'>重新参数化的句子空间</font>**

> 我们首先定义一个**<font color='red'> 循环语言模型的 句子空间(sentence space)</font>**，并展示这个模型如何将**一个给定的句子 映射到 句子空间中的一个轨迹(trajectory)**。
>
> 通过**将原始空间中的每个轨迹 映射到 新空间中的一个点**，将该 循环语言模型的句子空间 **重新参数化(reparametrize) 为 一个新的空间（<font color='red'> 重新参数化的句子空间，reparametrized sentence space)</font> **，
>
> > ==为了实现 重新参数化(reparametrize)：==
> >
> > 我们引入了两种互补的方法，**在经过训练和冻结语言模型中的每个时间步中，对 之前的隐藏状态 和 单元状态添加额外的偏差项**，并对这些**偏差项进行优化**，以最大化该句子的可能性。
>
> <font color='red'>**可恢复性(Recoverability)**不可避免地**取决于模型大小和底层语言模型的质量**</font>，因此我们随着重新参数化句子空间的不同维度而变化。我们发现**优化器的选择**（非线性共轭梯度优于随机梯度下降）和**初始化非常敏感**，因此简单的编码器设置不太可能开箱即用。
>
> 
>
> 我们的实验表明，==**具有重新参数化的句子空间，其维数等于模型的 循环隐藏状态的维数的 完全可恢复性**==，至少对于足够大的模型是这样：**对于几乎所有的句子，都存在一个向量，可以完美地恢复句子。**
>
> > 观察的结果：
> >
> > 1. 即使 句子 与 用于训练固定语言模型的句子 属于不同领域，这种趋势也适用。
> >
> > 2. **能够达到最大可恢复性的最小维数：**近似等于模型的 循环隐藏状态的维数。
> >
> > 3. 可恢复性随着句子长度的增加而降低，模型发现在句子后面生成单词越来越困难。
> >
> >    即：在解码一个给定的句子时，模型在生成一个错误的单词之后，很少会生成任何正确的单词。
> >
> > 4. 对恢复单词随机序列的实验表明：
> >
> >    我们的重新参数化的句子空间**不仅简单地记忆序列，还利用了语言模型**
> >
> >    这些观察结果表明，==无条件语言模型确实可以几乎完美地恢复任意句子==，并可能有未来作为通用解码器。



## **2** The Sentence Space of a Recurrent Language Model

### 2.1 The Sentence Space of a Recurrent Language Model 循环语言模型的句子空间

首先**介绍循环语言模型**的背景。然后我们描述它的句子空间并展示我们**如何重新参数化**它以便于分析。在这个重新参数化的句子空间中，我们**定义了一个句子的可恢复性**。

### 2.2 Defifining the Sentence Space 定义句子空间

在重新参数化的句子空间中，我们定义了一个句子的可恢复性。



等式2中的循环转换函数fθ定义了一个由一个句子中的标记（x1，……，xT）∈X的观察所驱动的动态系统

在这个动态系统中，所有的**轨迹**都从原点开始h0 =[0，…，0]，并随着时间的推移 根据传入的标记（xt）演化。

==任何**轨迹（h0，……，hT）**都完全嵌入到一个d维空间中，其中d 等于 隐藏状态的维数  且$ H∈ R^d$，即ht∈H。==

换句话说，语言模型**将一个长度为T的句子 嵌入一个D维空间H中的 T+1步轨迹**，我们将其称为 <font color='red'>**语言模型的句子空间 H**</font>



* **Reparametrizing the Sentence Space 重新参数化句子空间**

我们想**不对句子长度进行符号编码的 语义表征 中恢复句子**。鉴于此，**并且由于 一个中间标记的单一替换 就能极大地改变句子空间中的剩余轨迹**，我们想要一个==平坦的矢量表示==。



**<font color='red'>句子空间 $H∈R^d$ trajectories    --重新参数化--》  平面向量空间 $Z∈R^{d'}$</font>** 			（d 等于 隐藏状态的维数）

> 为了解决这个问题，我们提出（近似地）**将句子空间 重新参数化为 一个平面向量空间 Z∈Rd'：描述一个语言模型的句子空间**。
>
> 在建议的**重新参数化**方案下，**句子空间H**中的 **隐藏状态的轨迹** 映射到 **重新参数化的 句子空间Z**中的一个维数为d'的向量
>
> 为了实现他，我们在模型的**每个时间步中向前面的隐藏状态**和**单元格状态**添加**偏差项 bias**，并对它们进行优化，**使句子的对数概率最大化**，如图1所示。
>
> <img src="https://cdn.jsdelivr.net/gh/xin-fight/note_image@main/img/image-20230212211636119.png" alt="image-20230212211636119" style="zoom:50%;" />
>
> **我们添加这个偏差bias在两个方面：** 
>
> > ***模型参数 d∗ = 2*dl**：For a LSTM language model **with *l* layers of *d* LSTM units**, its model dimension *d∗* = 2*dl* because LSTMs have two hidden state vectors (conventionally h and c) both of dimension *d*.
> >
> > ***平面向量空间 Z ∈ Rd’***：In order to address this, we propose to (approximately) reparametrize the sentence space into **a flat-vector space Z ∈ Rd’** to characterize the sentence space of a language
>
> > (1)如果d'≤d∗，我们使用一个**随机投影矩阵** 来将我们的 **向量z∈Rd‘ 投影提到 d∗**
> >
> > (2)如果d‘>d∗，我们使用**软注意与之前的隐藏状态** 自适应将**向量z∈Rd 投影降到 d∗** (Bahdanau et al., 2015).



我们的重新参数化必须大致允许我们通过语言模型 **在标记序列 (x1, . . . , xT )  和 重新参数化空间 Z 中的点 z 之间 来回切换**： 向后（前向估计）和向前（后向估计）

> 一旦建立了这种来回属性，我们就**可以检查 Z 中的一组点而不是 H 中的轨迹**
>
> ==向量 z ∈ Z  类似于充当 **条件生成任务上下文的编码器的输出**。==**这使得Z中的分析 类似于 序列模型上的上下文分析**，从而帮助我们理解无条件的语言模型让我们正试图更好地适应z
>
> 我们期望来回切换，因为我们**期望z包含序列的所有信息**。由于我们在**每个时间步都添加z**，保存在z中的信息不会像处理序列那样快地退化，就像我们将其添加到初始隐藏和单元格状态那样。虽然还有其他类似的方法来集成z，但我们选择 修改循环连接。



* **Using the Sentence Space** 使用句子空间

在本文中，我们将语言模型的重新参数化的句子空间 **Z描述为一组$d'$维的向量**，==它对应于 **未用于训练**基础语言模型 **的一组D‘的句子。**==

> 这种对**未见过的句子的使用有助于我们从泛化而非记忆的角度来理解语言模型的句子空间**，从而深入了解将预训练的语言模型作为固定解码器/生成器的潜力。
>
> 使用我们的重新参数化句子空间框架，为调查词向量而设计的评价技术变得适用。
>
> > 我们现在可以做的那些有趣的技术之一是在我们的重参数化句子空间的不同句子之间进行插值（Choi等人，2017；Bowman等人，2016中的表1），但我们在此不做探讨。



* **Forward Estimation** 前向估计X→Z

**X→Z前向估计的目标是：**通过训练的语言模型（即固定的θ）==**找到一个点z∈Z，代表句子（x1，…，xT）∈X**==。			（d*为模型参数）

> 当z的维数小于模型维数d∗时（d'≤d∗），我们使用 随机投影矩阵 将其投影到d∗
>
> 当z的维数大于模型维数时（d‘>d∗），我们使用软注意矩阵将其投影到d∗。

我们修改了等式中的循环动态 fθ(2)：

> <img src="https://cdn.jsdelivr.net/gh/xin-fight/note_image@main/img/image-20230213011400839.png" alt="image-20230629030951537" style="zoom:80%;" />
>
> 变为
>
> <img src="https://cdn.jsdelivr.net/gh/xin-fight/note_image@main/img/image-20230213023947763.png" alt="image-20230629030951537" style="zoom:80%;" />
>
> > 其中$Z∈R^{d×k}$  仅仅是由 维数为$d$ 的$k = dim (z)/d$向量 组成的z的非平坦矩阵。		$W_z ∈ R^{d×d'}$
> >
> > 我们通过在这个修正模型下 **最大化给定句子的对数概率来估计z**，同时固定原始参数θ： 
> >
> > <img src="https://cdn.jsdelivr.net/gh/xin-fight/note_image@main/img/image-20230213015239598.png" alt="image-20230629030951537" style="zoom:80%;" />
> >
> > 这个目标函数是高度非凸的，有可能导致多个近似最优的z。因此，为了在正向估计中估计z，我们使用SciPy（Jones et al.，2014）实现的非线性共轭梯度（Wright和Nocedal，1999），限制为10000次迭代，尽管几乎所有运行收敛都要快得多。
> >
> > 
> >
> > **我们的实验表明，这些z中的许多在恢复原始句子时导致了相似的表现**



* **Backward Estimation** 后向估计Z→X

**Z→X反向估计是序列解码的一个实例**，旨在 **给定重新参数化的句子Z中的一个点z 恢复原始句子（x1，…，xT）**，我们称之为恢复。

我们**使用与公式（6）相同的目标函数**，但我们是对（x1, ... , xt）而不是对z进行优化。

与前向估计不同，后向估计是一个组合优化问题，不能用递归语言模型轻易求解 --- 为了避免这个问题，==**使用 beam search**==

### 2.3 Analyzing the Sentence Space through Recoverability 通过可恢复性分析句子空间

作为理解语言模型的句子空间的第一步，我们提出了**三个往返的可恢复性度量标准**，并描述了我们如何使用它们来描述句子空间。

* **Recoverability** 可恢复性

可恢复性是：衡量原句x = (x1, ... , xt ) ∈ X的信息 在重新参数化的句子空间Z中被保留了多少。

> 通过重构原始句子x测量：**首先，我们通过公式（6）从x∈X向前估计句子向量z∈Z。然后，我们通过后向估计，从估计的z重建句子$\hat{x}$。**
>
> 使用下面三种方法比较重建句子和原始句子：
>
> <img src="https://cdn.jsdelivr.net/gh/xin-fight/note_image@main/img/image-20230213015613194.png" alt="image-20230213015613194" style="zoom:67%;" />
>
> 我们**使用前缀匹配**，因为早期的实验表明，在生成质量方面有一个非常强烈的从左到右的衰减。换句话说，候选生成器对较短的句子更好，一旦生成了一个错误的标记，未来的标记就极不可能是正确的。



* **Effective Dimension by Recoverability** 按可恢复性划分的有效维度

==<font color='blue'>如果所有的句子都能投影到一个d维的句子空间Z中并完美地恢复，则**Z的有效维数必须不大于d。**</font>==

我们关注给定目标可恢复性τ 的有效维度：		($D'$是未经过训练的句子)

> <img src="https://cdn.jsdelivr.net/gh/xin-fight/note_image@main/img/image-20230213120105538.png" alt="image-20230629030951537" style="zoom:80%;" />
>
> 换句话说，给定一个训练好的模型(θ)，我们就找到了**满足目标可恢复性（τ）的最小有效维数$d'$（Z的维数)。**
>
> 使用它，我们可以回答在θ模型下实现可恢复性τ所需的最小维数d'等问题。使用此，**无约束的有效维数，即==满足 最佳可恢复性的 最小维为==**
>
> <img src="https://cdn.jsdelivr.net/gh/xin-fight/note_image@main/img/image-20230213120959306.png" alt="image-20230629030951537" style="zoom:80%;" />



由于我们的前向估计过程使用非凸优化，而我们的后向估计过程使用波束搜索，因此我们的有效维数估计是上界近似。

## 4 Results and Analysis

* **Recoverability Results**

**<font color='red'>我们观察到，可恢复性随着d'的增加而增加，直到d'=d∗。在此之后，可恢复性趋于稳定。</font>**

==<font color='red'>可恢复性随着语言**模型的大小和质量**以及**重新参数化句子空间维度**的增加而提高。</font>== （图二可得）

我们发现，随着语言模型的训练，在特定d‘下的可恢复性增加

* **Effective Dimension of the Sentence Space**

对于其他模型大小和重参数化空间的其他维度，我们不能完美地恢复一些句子。为了确定哪些句子不能恢复，我们看每条曲线的形状：

> 我们观察到，这些曲线中的绝大多数从未增加过，**这表明可恢复性和句子长度有很强的负相关关系**。当句子长度超过30时，大多数曲线下降到0，说明较长的句子更难恢复。早期使用神经序列到序列模型进行机器翻译的观察正好得出了这个结论（Cho et al.，2014；Koehn和Knowles，2017）。

这表明，<font color='red'>固定长度的表示 缺乏表示 复杂句子的能力，可能会牺牲重要的信息来编码其他句子。</font>

可恢复性的下降也意味着，==**句子空间的无约束有效维度可能 与 句子的长度密切相关，而可能与模型维度d∗无关**==。较小的模型的无约束有效维度远大于d∗的事实支持了这一说法。

* **Impact of Beam Width & Optimization Strategy**

> 为了分析各种beam宽度的影响，我们在解码中试验了5、10和20的波束宽度。我们发现，**这些beam宽度的结果是一致的**。因此，本文中除此以外的所有实验结果都使用5的波束宽度。我们在表2中提供了一个有代表性的句子可恢复性的部分表格，在解码过程中只改变了波束宽度。

> 我们发现，使用Adam对所有三种情况的恢复估计差距都不超过1.0 BLEU，这暗示了优化问题的高度非凸性。

* **Sources of Randomness** 随机性来源

有两点随机性：**前向估计中的优化程序的非凸性（公式6）** 和 **随机投影矩阵Wz的采样**。

> 然而，基于图2中的小标准差表面，这些数据对可恢复性的影响很小。
>
> 此外，对可恢复性的高置信度（低方差）上限估计的观察，支持了我们的**可恢复性度量 在对语言模型的句子空间的调查中 的可用性**。

* **Out-of-Domain Recoverability**  域外可恢复性

图3中的左图和中图显示，在d‘≥d∗时，即使对于中大型模型的域外句子，BLEU测量的恢复性能也几乎是完美的，遵循图2中对英语千字的实验的趋势

* **More than just Memorization** 不仅仅是记忆

在域外句子上近乎完美的表现表明，**这种方法可能是 通过利用语言模型来 学习语言的重要属性，或者只是在不使用语言模型的情况下 记忆任何任意序列**。

经过实验表明，记忆并不能完全解释Gigaword或IWSLT16的结果。

* **Towards a General-Purpose Decoder **面向通用解码器

在这种表述中，我们的**向量z’可以被认为是可训练的语境，用来训练我们的无条件语言模型，以生成任意的句子。**

> 由于我们发现合理大小的训练有素的语言模型有一个不受约束的有效维度，其具有很高的可恢复性，在域内和域外序列上都近似于其模型维度，**因此无条件的语言模型能够有效地利用我们的语境z‘。**
>
> 进一步的实验证实，**我们的语境向量并不是简单地记忆任意的序列，而是利用语言模型来产生良好形式的序列**。因此，这样的模型在给定一个有能力生成最佳的语境向量z'的编码器，可以被用作独立于任务的解码器

我们观察到，小型和中型模型的可恢复性并不完美，对于较长的句子来说，可恢复性急剧下降，**表明高可恢复性的最小模型规模相当大。**

我们的方法可以使**用一个正则化机制 来平滑隐含的句子空间**。这可以提高可恢复性并减少无约束的有效维度，从而提高无条件语言模型作为通用解码器的适用性。

## 5 Related Work

* **Latent Variable Recurrent Language Models** 潜变量递归语言模型

我们描述语言模型的句子空间的方式可以被认为是使用一个固定的解码器θ对一个隐含的潜变量z进行推理。

我们的方法与这些方法不同的是，我们完全专注于分析一个无条件训练的固定模型。我们对句子空间的表述也更加普遍，有可能适用于所有这些模型。

* **Pretrained Recurrent Language Models**

预训练或单独训练的语言模型基本上被用于两种情况：作为下游任务的特征提取器和作为特定任务解码器的评分函数（Gulcehre等人，2015；Li等人，2016；Sriram等人，2018）。

以上都没有分析预训练的模型如何代表句子，也没有研究使用语言模型作为解码器的潜力。

## 6 Conclusion

为了回答无条件的语言模型是否可以有条件地生成被保留的句子，我们为一个冻结的、预训练的语言模型引入了重新参数化的句子空间的概念，其中每个句子被表示为一个点向量，它被添加为一个偏置，并在解码期间被优化以重现该句子。

我们设计了**基于优化的前向估计和基于波束搜索的后向估计程序**，使我们能够将一个句子映射到重新参数化的空间，并从该空间中取出。然后，我们引入并使用**可恢复性指标**，使我们能够测量重新参数化空间的有效维度，并发现句子在多大程度上可以通过模型从固定大小的表征中恢复出来，而无需进一步训练。



我们发现**可恢复性随着重构空间维度的增加而增加**，直到达到模型维度，之后，对于训练有素的、足够大的（d≥512）模型，可恢复性趋于平稳。



这些实验揭示了**语言模型的句子空间的两个特性**。

> 首先，可恢复性随着语言模型的大小和质量的提高而提高，<font color='red'>**当重新参数化空间的维度等于模型的维度时，可恢复性几乎是完美的。**</font>
>
> 其次，**可恢复性与句子的长度呈负相关**，也就是说，长句子的可恢复性更难。我们基于可恢复性的句子空间分析方法给出了空间有效维度的保守估计（上界）和相关可恢复性的下界。



我们认为有三个途径可以进一步开展工作。

> 衡量正则化（鼓励重新参数化的句子空间具有某种形式）和非线性之间的关系将是有价值的。
>
> 此外，尽管我们的框架与下游任务和网络架构无关，但我们希望比较可恢复性和下游任务性能，并分析不同架构的语言模型的句子空间。
>
> 我们还想利用这个框架来转换编码器的表征，以用于数据和记忆效率高的条件生成模型。
