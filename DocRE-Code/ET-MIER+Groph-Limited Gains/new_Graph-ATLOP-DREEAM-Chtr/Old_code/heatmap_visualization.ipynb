{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7540ca83",
   "metadata": {},
   "source": [
    "## Helper functions for writing LaTeX files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f45010e-9d79-41a2-960d-a3c6a0d4c210",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference: https://github.com/jiesutd/Text-Attention-Heatmap-Visualization\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "latex_special_token = [\"!@#$%^&*()\"]\n",
    "\n",
    "def generate(text_list, attention_list, sent_pos, new_mentions, color='red', rescale_value = False):\n",
    "    assert(len(text_list) == len(attention_list))\n",
    "    if rescale_value:\n",
    "        attention_list = rescale(attention_list)\n",
    "    word_num = len(text_list)\n",
    "    text_list = clean_word(text_list)\n",
    "    string = ''\n",
    "    for idx in range(word_num):\n",
    "        if idx + 1 in sent_pos:\n",
    "            string += f'[{sent_pos[idx + 1]}]'\n",
    "        if idx in new_mentions:\n",
    "             string += \"\\\\colorbox{%s!%.3f}{\"%(color, attention_list[idx])+\"\\\\strut \" + \"\\\\textbf{\" + text_list[idx]+\"}} \"\n",
    "        else:\n",
    "            string += \"\\\\colorbox{%s!%.3f}{\"%(color, attention_list[idx])+\"\\\\strut \" + text_list[idx]+\"} \"\n",
    "            \n",
    "    string += '\\n'\n",
    "    return string\n",
    "\n",
    "def rescale(input_list):\n",
    "    the_array = np.asarray(input_list)\n",
    "    the_max = np.max(the_array)\n",
    "    the_min = np.min(the_array)\n",
    "    rescale = (the_array - the_min)/(the_max-the_min)*100\n",
    "    return rescale.tolist()\n",
    "\n",
    "\n",
    "def clean_word(word_list):\n",
    "    new_word_list = []\n",
    "    for word in word_list:\n",
    "        for latex_sensitive in [\"\\\\\", \"%\", \"&\", \"^\", \"#\", \"_\",  \"{\", \"}\"]:\n",
    "            if latex_sensitive in word:\n",
    "                word = word.replace(latex_sensitive, '\\\\'+latex_sensitive)\n",
    "        new_word_list.append(word)\n",
    "    return new_word_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ba72f6",
   "metadata": {},
   "source": [
    "## Load Model and make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804cba3a-4d8c-472d-83e8-18a44a46f6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import torch\n",
    "from transformers import AutoConfig, AutoModel, AutoTokenizer\n",
    "from model import DocREModel\n",
    "from prepro import read_docred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62fa9dec-11c9-4269-b034-8189a92af39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_name_or_path = \"bert-base-cased\"\n",
    "num_class = 97\n",
    "num_labels = 4\n",
    "max_sent_num = 25\n",
    "evi_thresh = 0.2\n",
    "transformer_type = \"bert\"\n",
    "\n",
    "config = AutoConfig.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    num_labels=num_class,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name_or_path,\n",
    ")\n",
    "model = AutoModel.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    from_tf=bool(\".ckpt\" in model_name_or_path),\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "config.transformer_type = transformer_type\n",
    "\n",
    "read = read_docred    \n",
    "config.cls_token_id = tokenizer.cls_token_id\n",
    "config.sep_token_id = tokenizer.sep_token_id\n",
    "\n",
    "model = DocREModel(config, model, tokenizer,\n",
    "                num_labels=num_labels,\n",
    "                max_sent_num=max_sent_num, \n",
    "                evi_thresh=evi_thresh)\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc717220-5e67-4501-8511-658e2663c7cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "load_path = '/path/to/trained/model'\n",
    "model_path = os.path.join(load_path, \"best.ckpt\")\n",
    "model.load_state_dict(torch.load(model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c06a559a-1e41-4636-8053-7ed091b5abde",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_file = '/path/to/file/to/predict'  \n",
    "max_seq_length = 1024\n",
    "test_features = read(test_file, tokenizer, transformer_type=transformer_type, max_seq_length=max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b94e17-b4d0-43a0-9cde-14673e8022da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from utils import collate_fn\n",
    "from run import load_input\n",
    "\n",
    "def evaluate(model, features, tag=\"infer\"):\n",
    "    \n",
    "    dataloader = DataLoader(features, batch_size=1, shuffle=False, collate_fn=collate_fn, drop_last=False)\n",
    "    preds, evi_preds = [], []\n",
    "    scores, topks = [], []\n",
    "    toks, attns = [], []\n",
    "    \n",
    "    for batch in dataloader:\n",
    "        model.eval()\n",
    "        \n",
    "        inputs = load_input(batch, device, tag)\n",
    "        \n",
    "        toks.append(inputs['input_ids'])\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            pred = outputs[\"rel_pred\"]\n",
    "            pred = pred.cpu().numpy()\n",
    "            pred[np.isnan(pred)] = 0\n",
    "            preds.append(pred)\n",
    "\n",
    "            if \"scores\" in outputs:\n",
    "                scores.append(outputs[\"scores\"].cpu().numpy())  \n",
    "                topks.append(outputs[\"topks\"].cpu().numpy())   \n",
    "\n",
    "            if \"evi_pred\" in outputs: # relation extraction and evidence extraction\n",
    "                evi_pred = outputs[\"evi_pred\"]\n",
    "                evi_pred = evi_pred.cpu().numpy()\n",
    "                evi_preds.append(evi_pred)   \n",
    "            \n",
    "            if \"attns\" in outputs: # attention recorded\n",
    "                attn = outputs[\"attns\"]\n",
    "                attns.extend([a.cpu().numpy() for a in attn])\n",
    "        \n",
    "    return toks, attns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad6f6f5-29d2-444e-b42b-7d6830c6fb80",
   "metadata": {},
   "outputs": [],
   "source": [
    "words, attns = evaluate(model, test_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f10302e",
   "metadata": {},
   "source": [
    "## Visualization by writing into a LaTeX file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51fe99fe-74cb-4322-885c-e5c5be37c741",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_id = 925"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec37f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_toks = words[doc_id][0]\n",
    "curr_attns = attns[doc_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3315306",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features[doc_id].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5957e1ea-8158-40e1-b5b8-91cc1d90e361",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_attns = [attn for attn in curr_attns if attn.shape[0] != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac32595-df9a-4d05-b3ac-7cdfb64d9b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get gold relations.\n",
    "\n",
    "valid_rels = []\n",
    "\n",
    "for i, one_hot in enumerate(test_features[doc_id][\"labels\"]):\n",
    "    valid = [j for j in range(1, num_class) if one_hot[j] != 0]\n",
    "    if valid:\n",
    "        valid_rels.append((i, valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92122cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get name of relations.\n",
    "\n",
    "docred_rel2id = json.load(open('meta/rel2id.json', 'r'))\n",
    "id2name = json.load(open('meta/rel_info.json'))\n",
    "\n",
    "docred_id2rel = {v: id2name[k] for (k,v) in docred_rel2id.items() if k in id2name}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5909af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tok2word(toks, attns, sent_pos, ments):\n",
    "    \n",
    "    words = []\n",
    "    new_attns = []\n",
    "    old2new = {}\n",
    "    \n",
    "    for tid, tok in enumerate(toks):\n",
    "        \n",
    "        if tok in ['[CLS]', '[SEP]']:\n",
    "            continue\n",
    "            \n",
    "        if tok.startswith('##'):\n",
    "            words[-1] += tok.strip('##')\n",
    "            new_attns[-1] += attns[tid]\n",
    "            old2new[tid] = len(words)\n",
    "        \n",
    "        else:\n",
    "            words.append(tok)\n",
    "            new_attns.append(attns[tid])\n",
    "            old2new[tid] = len(words)\n",
    "            \n",
    "    new_attns = new_attns/sum(new_attns)\n",
    "\n",
    "    new_ments = []\n",
    "    \n",
    "    for ment in ments:\n",
    "        new_ments.extend(range(old2new[ment[0]], old2new[ment[1]]))\n",
    "        \n",
    "    new_sent_pos = {}\n",
    "    \n",
    "    for sid, pos in enumerate(sent_pos):\n",
    "        new_sent_pos[old2new[pos[0] + 1]] = sid + 1\n",
    "        \n",
    "    return words, new_attns, new_sent_pos, new_ments, old2new\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ad007c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rels(words, head, tail, old2new, rels, evi):\n",
    "    \n",
    "    head = words[old2new[head[0][0]+1]: old2new[head[0][1]-1]]\n",
    "    tail = words[old2new[tail[0][0]+1]: old2new[tail[0][1]-1]]\n",
    "    info = \"\\\\textbf{subject}:\" + ' '.join(head) + \"; \\\\textbf{object}:\" + ' '.join(tail) + \\\n",
    "            \"; \\\\textbf{relation}:\" + ', '.join(rels) + \"; \\\\textbf{evidence}:\" \n",
    "    \n",
    "    for e in evi:\n",
    "        info += str(e) + ','\n",
    "    \n",
    "    info = info[:-1]\n",
    "    info += '\\n'\n",
    "    \n",
    "    return info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c75d5b-ed49-4c3b-843d-7e90cd4d2eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "latex_file = 'toy.tex'\n",
    "\n",
    "with open(latex_file, 'w') as f:\n",
    "    f.write(r'''\\documentclass{standalone}\n",
    "\\usepackage{color}\n",
    "\\usepackage{tcolorbox}\n",
    "\\usepackage{CJK}\n",
    "\\usepackage{adjustbox}\n",
    "\\tcbset{width=0.9\\textwidth,boxrule=0pt,colback=red,arc=0pt,auto outer arc,left=0pt,right=0pt,boxsep=5pt}\n",
    "\\begin{document}\n",
    "\\begin{CJK*}{UTF8}{gbsn}'''+'\\n')\n",
    "    string = r'''{\\setlength{\\fboxsep}{0pt}\\colorbox{white!0}{\\parbox{0.9\\textwidth}{'''+\"\\n\"\n",
    "    \n",
    "    for rid, rel in valid_rels:\n",
    "    \n",
    "        curr_rels =  [docred_id2rel[r] for r in rel]\n",
    "        curr_doc_toks = tokenizer.convert_ids_to_tokens(curr_toks)\n",
    "        curr_doc_attn = curr_attns[rid]\n",
    "        curr_sent_pos = test_features[doc_id][\"sent_pos\"]\n",
    "        curr_evi = [sid + 1 for sid, val in enumerate(test_features[doc_id][\"sent_labels\"][rid]) if val != 0]\n",
    "        curr_ht = test_features[doc_id][\"hts\"][rid]\n",
    "        curr_head = test_features[doc_id][\"entity_pos\"][curr_ht[0]]\n",
    "        curr_tail = test_features[doc_id][\"entity_pos\"][curr_ht[1]]\n",
    "        mentions = curr_head + curr_tail\n",
    "        curr_doc_words, curr_doc_attn, sent_pos, new_mentions, old2new = tok2word(curr_doc_toks, curr_doc_attn, curr_sent_pos, mentions)\n",
    "        curr_rels = get_rels(curr_doc_words, curr_head, curr_tail, old2new, curr_rels, curr_evi)\n",
    "        string += curr_rels + \"\\n\"\n",
    "            \n",
    "        OldRange = curr_doc_attn.max() - curr_doc_attn.min()\n",
    "        NewRange = 100  \n",
    "        NewValue = (((curr_doc_attn - curr_doc_attn.min()) * NewRange) / OldRange) + 0\n",
    "        string += generate(curr_doc_words, NewValue, sent_pos, new_mentions, \"red\")\n",
    "        string += '\\n'\n",
    "    string += \"\\n}}}\"\n",
    "    f.write(string +'\\n')\n",
    "    f.write(r'''\\end{CJK*}\n",
    "\\end{document}''')\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb393d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "docre",
   "language": "python",
   "name": "docre"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
